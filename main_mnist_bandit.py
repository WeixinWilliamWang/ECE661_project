import argparse
import os
from pathlib import Path
import numpy as np
import torch
import matplotlib.pyplot as plt
from typing import Union, Tuple, Optional

import torch.nn as nn
import matplotlib as mpl
import umap.umap_ as umap

from diffusion_TS import DiffusionPriorMLP
from bandit import (
    LinBandit,
    evaluate,
    LinTS,
    LinDiffTS,
    LinDiffDPTS,
    LinDiffDPS,
    LinDiffDMAP,
    NeuralTS,
    NeuralUCB,
    NeuralDiffDPTS,
    NeuralDiffDPS,
    NeuralDiffDMAP,
)


mpl.style.use("classic")
mpl.rcParams["figure.figsize"] = [6, 4]

mpl.rcParams["axes.linewidth"] = 0.75
mpl.rcParams["errorbar.capsize"] = 3
mpl.rcParams["figure.facecolor"] = "w"
mpl.rcParams["grid.linewidth"] = 0.75
mpl.rcParams["lines.linewidth"] = 0.75
mpl.rcParams["patch.linewidth"] = 0.75
mpl.rcParams["xtick.major.size"] = 3
mpl.rcParams["ytick.major.size"] = 3

mpl.rcParams["pdf.fonttype"] = 42
mpl.rcParams["ps.fonttype"] = 42
mpl.rcParams["font.size"] = 10
mpl.rcParams["axes.titlesize"] = "medium"
mpl.rcParams["legend.fontsize"] = "medium"



mpl.rcParams['savefig.bbox'] = 'tight'
mpl.rcParams['savefig.pad_inches'] = 0



# ---- Imports from training script ----
from train_MNIST import (
    load_full_mnist,
    generate_theta_distribution,
)

# ---- Utils to load linear / MLP params ----


def load_linear_params(path: Path) -> np.ndarray:
    """Load θ samples matrix saved by train_MNIST.py (shape N × d)."""
    assert path.exists(), f"Linear θ file {path} not found. Run train_MNIST.py first."
    return np.load(path)


def flatten_state_dict(sd: dict[str, torch.Tensor]) -> np.ndarray:
    """Flatten state_dict parameters to 1-D numpy array."""
    vec = torch.cat([p.view(-1) for p in sd.values()])
    return vec.cpu().numpy()


def load_mlp_param_matrix(pt_file: Path) -> np.ndarray:
    """Load list of state_dicts and return matrix (N × P)."""
    assert pt_file.exists(), f"MLP params file {pt_file} missing."
    sd_list = torch.load(pt_file, map_location="cpu")
    mat = np.stack([flatten_state_dict(sd) for sd in sd_list]).astype(np.float32)
    return mat










class SmallMLP(nn.Module):
    """Simple binary MLP: 784  -> 128 -> 1"""

    def __init__(self, in_dim: int = 784):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 128), nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.net(x).squeeze(1)  # output shape (batch,)



class MNISTNeuralBandit(LinBandit):
    """Bandit where rewards are generated by a binary MLP drawn from neural parameter set.

    For each episode a parameter vector is sampled from `neural_param_matrix` (test split).
    Rewards are sign(MLP(x))."""

    def __init__(self, features: np.ndarray, labels: np.ndarray, sigma: float = 0.0):
        # --- Dataset & dimensions ---
        self.labels = labels.astype(np.int64, copy=False)
        self.all_features = features.astype(np.float32, copy=False)
        self.d = features.shape[1]  # 28*28 = 784

        # --- Bandit arms configuration ---
        self.K = 100  # number of arms per round

        # --- Positive class (chosen once per environment) ---
        self.pos_class = int(np.random.randint(0, 10))

        self.sigma = sigma

        # template MLP (if we later want neural rewards); keep for compatibility
        self.template = SmallMLP(in_dim=self.d)

        # Parent LinBandit needs placeholders; will be overwritten in randomize()
        # Pass labels to parent constructor so that it is available when randomize() is first called
        super().__init__(np.zeros((self.K, self.d)), np.zeros(self.d), sigma=sigma, labels=self.labels)

    @staticmethod
    def vec_to_state_dict(vec: np.ndarray, template: nn.Module) -> dict[str, torch.Tensor]:
        sd = {}
        offset = 0
        for name, param in template.state_dict().items():
            numel = param.numel()
            sd[name] = torch.from_numpy(vec[offset:offset+numel]).view_as(param)
            offset += numel
        return sd

    def randomize(self):
        # Choose K random images as arms
        arms_idx = np.random.choice(self.all_features.shape[0], size=self.K, replace=False)
        self.X = self.all_features[arms_idx]
        arm_labels = self.labels[arms_idx]

        # Reward structure: +1 for positive class, -1 otherwise
        self.mu = np.where(arm_labels == self.pos_class, 1.0, -1.0)

        # Optionally add Gaussian noise
        if self.sigma > 0:
            self.rt = self.mu + self.sigma * np.random.randn(self.K)
        else:
            self.rt = self.mu.copy()

        # Identify best arm (any with +1, choose first)
        pos_indices = np.where(arm_labels == self.pos_class)[0]
        self.best_arm = int(pos_indices[0]) if pos_indices.size > 0 else 0


def prepare_mnist_pixels():
    """Load raw MNIST pixel vectors (flattened 784)."""
    images, labels = load_full_mnist(flatten=True)
    return images.numpy().astype(np.float32), labels.numpy().astype(np.int64)


# ----------------- Plot helpers -----------------
def linestyle2dashes(style: str) -> Union[str, Tuple[int, ...]]:
    """Map matplotlib linestyle to dash pattern or return the style itself.

    Returns a tuple for custom dash sequences that can be passed to
    Line2D.set_dashes(). For solid lines returns "-" etc.
    """
    mapping = {
        "-": "-",           # solid
        "--": (6, 2),        # dashed
        "-.": (4, 2, 1, 2),  # dash-dot
        ":": (1, 2),         # dotted
    }
    return mapping.get(style, style)

def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main(args):
    set_seed(args.seed)
    # set paths
    lin_theta_path = os.path.join(args.output_dir, "mnist_linear_thetas.npy")
    mlp_param_path = os.path.join(args.output_dir, "mnist_mlp_state_dicts.pt")
    os.makedirs(args.output_dir, exist_ok=True)
    results_dir = os.path.join(args.output_dir, "eval_results")
    os.makedirs(results_dir, exist_ok=True)

    # ---- linear θ matrix ----
    if lin_theta_path.exists():
        print(f"Loading linear θ samples from {lin_theta_path}")
        theta_samples = np.load(lin_theta_path)
        d_linear = theta_samples.shape[1]
        d_feat = d_linear  # feature dimension now equals θ dimension (using raw pixels)

        # Always load raw pixel data to form bandit arms
        ds_path = os.path.join(args.output_dir, "mnist_features_labels.npz")
        if ds_path.exists():
            # Use memory mapping to avoid loading the entire array into RAM
            npz = np.load(ds_path, mmap_mode='r')
            if "features" in npz.files and "labels" in npz.files:
                features_all = npz["features"]   # memmap view (float32 expected)
                labels_all   = npz["labels"]     # memmap view (int64 expected)
                print(f"[Cache] Loaded pre-extracted MNIST features (memmap) from {ds_path}")
            else:
                print(f"[Warning] {ds_path} missing keys; regenerating dataset.")
                ds_path.unlink(missing_ok=True)
                images_px, lbls_px = load_full_mnist(flatten=True)
                features_tmp = images_px.numpy().astype(np.float32)
                labels_tmp   = lbls_px.numpy().astype(np.int64)
                np.savez(ds_path, features=features_tmp, labels=labels_tmp)
                # Re-open as memmap to keep RAM usage low
                npz = np.load(ds_path, mmap_mode='r')
                features_all = npz["features"]
                labels_all   = npz["labels"]
                print(f"Saved and memory-mapped MNIST features to {ds_path}")
        else:
            images_px, lbls_px = load_full_mnist(flatten=True)
            features_tmp = images_px.numpy().astype(np.float32)
            labels_tmp   = lbls_px.numpy().astype(np.int64)
            np.savez(ds_path, features=features_tmp, labels=labels_tmp)
            # Re-open as memmap to keep RAM usage low
            npz = np.load(ds_path, mmap_mode='r')
            features_all = npz["features"]
            labels_all   = npz["labels"]
            print(f"Saved and memory-mapped MNIST features to {ds_path}")
        # Try to load positive‐class labels if available (for visualization)
        pos_path = Path(args.output_dir) / "pos_labels.npy"
        pos_labels = np.load(pos_path) if pos_path.exists() else None
    else:
        raise FileNotFoundError("Please run train_MNIST.py to generate linear parameter file.")

    # ---------------- train / test split (8000 / remainder) ----------------
    perm = np.random.permutation(theta_samples.shape[0])
    n_train_lin = min(8000, theta_samples.shape[0])
    theta_train = theta_samples[perm[:n_train_lin]]
    theta_test = theta_samples[perm[n_train_lin:]]

    pos_labels_train = pos_labels_test = None
    if pos_labels is not None:
        pos_labels = pos_labels[perm]
        pos_labels_train = pos_labels[:n_train_lin]
        pos_labels_test = pos_labels[n_train_lin:]

    # ----- Linear prior -----
    lin_prior_path = os.path.join(args.output_dir, "diffusion_prior_linear_sd.pt")
    device = args.device
    if lin_prior_path.exists():
        print(f"[Cache] Loading linear DiffusionPrior state_dict from {lin_prior_path}")
        lin_prior = DiffusionPriorMLP(d=d_feat, T=args.T, alpha=args.alpha,
                                      hidden=args.hidden_size, device=device,
                                      schedule=args.schedule, arch="mlp")
        state_dict = torch.load(lin_prior_path, map_location=device)
        lin_prior.load_state_dict(state_dict)
    else:
        lin_prior = DiffusionPriorMLP(d=d_feat, T=args.T, alpha=args.alpha,
                                      hidden=args.hidden_size, device=device,
                                      schedule=args.schedule, arch="mlp")
        lin_prior.train(theta_train, epochs=args.epochs, batch=args.batch, lr=args.lr)
        torch.save(lin_prior.state_dict(), lin_prior_path)


    if d_feat >=2:
        n_vis = min(1000, theta_test.shape[0])

        real_subset = theta_test[:n_vis]
        prior_subset = lin_prior.sample(n_vis)[0]

        reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1, random_state=0)
        real_2d = reducer.fit_transform(real_subset)
        prior_2d = reducer.transform(prior_subset)

        plt.figure(figsize=(6, 6))
        plt.scatter(real_2d[:, 0], real_2d[:, 1], s=6, marker=".", color="b", label="True Prior")
        plt.scatter(prior_2d[:, 0], prior_2d[:, 1], s=6, marker=".", color="grey", label="Diffusion Prior")
        plt.ylabel("True and diffusion priors")
        plt.tight_layout()
        plt.legend()
        plt.savefig(Path(args.output_dir) / "mnist_diffusion_prior.pdf")
        plt.close()

    # ----- Neural (MLP) prior -----
    neural_mat = load_mlp_param_matrix(mlp_param_path)
    P = neural_mat.shape[1]

    # ---- train / test split first (avoid leakage when computing μ,σ) ----
    # perm2 = rng_split.permutation(neural_mat.shape[0])
    perm2 = np.random.permutation(neural_mat.shape[0])
    n_train_neural = min(8000, neural_mat.shape[0])
    neural_train= neural_mat[perm2[:n_train_neural]]
    neural_test = neural_mat[perm2[n_train_neural:]]


    neural_prior_path = os.path.join(args.output_dir, "diffusion_prior_mlp_sd.pt")
    if neural_prior_path.exists():
        print(f"[Cache] Loading neural DiffusionPrior state_dict from {neural_prior_path}")
        neural_prior = DiffusionPriorMLP(
            d=P,
            T=args.T,
            alpha=args.alpha,
            hidden=args.hidden_size,
            device=args.device,
            schedule=args.schedule,
            arch="mlp",
        )
        state_dict = torch.load(neural_prior_path, map_location=args.device)
        neural_prior.load_state_dict(state_dict)
    else:
        # Training hyper-parameters
        train_epochs = args.epochs    # training iterations
        train_batch  = args.batch    # mini-batch size
        train_lr     = args.lr   # AdamW learning rate

        neural_prior = DiffusionPriorMLP(
            d=P,
            T=args.T,
            alpha=args.alpha,
            hidden=args.hidden_size,
            device=args.device,
            schedule=args.schedule,
            arch="mlp",
        )
        neural_prior.train(neural_train, epochs=train_epochs, batch=train_batch, lr=train_lr)
        torch.save(neural_prior.state_dict(), neural_prior_path)

    ######################## 2.5) Visualize prior vs true θ ###################
    if d_feat >= 2:

        # ----------- Visualize MLP parameter space -----------
        fig_path_mlp = Path(args.output_dir) / "mnist_mlp_diffusion_prior.pdf"
        if not fig_path_mlp.exists():
            n_vis_mlp = min(1000, neural_test.shape[0])
            real_mlp_subset = neural_test[:n_vis_mlp]
            prior_mlp_subset = neural_prior.sample(n_vis_mlp)[0]

            reducer_mlp = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1, random_state=0)
            real_mlp_2d = reducer_mlp.fit_transform(real_mlp_subset)
            prior_mlp_2d = reducer_mlp.transform(prior_mlp_subset)

            plt.figure(figsize=(6, 6))
            plt.scatter(real_mlp_2d[:, 0], real_mlp_2d[:, 1], s=6, marker=".", color="b", label="True Prior")
            plt.scatter(prior_mlp_2d[:, 0], prior_mlp_2d[:, 1], s=6, marker=".", color="grey", label="Diffusion Prior")
            plt.ylabel("True and diffusion priors")
            plt.tight_layout()
            plt.legend()
            plt.savefig(fig_path_mlp)
            plt.close()
        else:
            print(f"[Skip] {fig_path_mlp} already exists, skipping MLP prior plot.")

    ######################## 3) Bandit evaluation #############################
    rng = np.random.default_rng(0)
    envs = [MNISTNeuralBandit(features_all, labels_all, sigma=args.sigma) for _ in range(args.runs)]

    # Define all algorithms & their params

    neural_common = {
        "hidden": 128,
        "nu": 0.1,
        "batch_size": 128,
        "reduce": None,
        "reg": 0.1,
        "device": args.device,
    }

    neural_ts_params = dict(neural_common)
    neural_ucb_params = dict(neural_common)

    full_alg_list = {
        "LinTS": ("LinTS", {"sigma": args.sigma}, "tab:blue", "--", "TS"),
        "LinDiffTS": (
            "LinDiffTS",
            {
                "prior": lin_prior,
                "sigma": args.sigma,
                "theta0": np.zeros(d_feat),
                "Sigma0": 1e6 * np.eye(d_feat),
            },
            "tab:orange",
            "-",
            "DiffTS",
        ),
        "NeuralTS": ("NeuralTS", neural_ts_params, "tab:green", "--", "NeuralTS"),
        "NeuralUCB": ("NeuralUCB", neural_ucb_params, "tab:red", "-.", "NeuralUCB"),
        "NeuralDiffDPTS": (
            "NeuralDiffDPTS",
            {
                "prior": neural_prior,
                "sigma": args.sigma,
                "num_steps_sgld": args.num_steps_sgld,
                "step_size_sgld": args.step_size_sgld,
                "noise_scale": args.noise_scale,
            },
            "tab:purple",
            "-",
            "DLTS",
        ),
        "NeuralDiffDMAP": (
            "NeuralDiffDMAP",
            {
                "prior": neural_prior,
                "sigma": args.sigma,
                "K_inner": args.K_inner,
                "eta": args.dmap_eta,
            },
            "tab:olive",
            "--",
            "DMAP",
        ),

    }

    available_algs = list(full_alg_list.keys())
    alg_arg = args.alg.strip().lower()
    if alg_arg == "all":
        selected_alg_keys = available_algs
    else:
        requested = [item.strip() for item in args.alg.split(",") if item.strip()]
        invalid = sorted(set(requested) - set(available_algs))
        if invalid:
            raise ValueError(
                f"Unknown algorithms requested: {invalid}. Available options: {available_algs}"
            )
        selected_alg_keys = requested

    if not selected_alg_keys:
        raise ValueError(f"No algorithms selected. Available options: {available_algs}")

    algs = [full_alg_list[name] for name in selected_alg_keys]

    step = np.arange(1, args.horizon + 1)
    # subset indices where we show error bars
    sube = np.linspace(0, args.horizon - 1, num=10, dtype=int)

    for alg in algs:
        alg_class = globals()[alg[0]]
        # Log which algorithm starts to help locate memory bottlenecks
        print(f"[RUN] {alg[0]} ({alg[4]})", flush=True)
        # Run evaluation sequentially
        regret, _ = evaluate(alg_class, alg[1], envs, args.horizon, printout=True)
        print(f"[DONE] {alg[0]} ({alg[4]})", flush=True)

        cum_reg = regret.cumsum(axis=0)

        # Persist evaluation results for later analysis/plotting
        alg_name = alg[0]
        result_path = results_dir / f"{alg_name}_results.npz"
        np.savez_compressed(
            result_path,
            regret=regret,
            cumulative_regret=cum_reg,
            step=step,
            algorithm=np.array(alg_name),
            display_label=np.array(alg[4]),
            runs=np.array(args.runs, dtype=np.int32),
            horizon=np.array(args.horizon, dtype=np.int32),
            sigma=np.array(args.sigma, dtype=np.float32),
        )

        style = linestyle2dashes(alg[3])

        line_kwargs = dict(color=alg[2], label=alg[4], linewidth=2)
        if isinstance(style, tuple):
            line, = plt.plot(step, cum_reg.mean(axis=1), **line_kwargs)
            line.set_dashes(style)
        else:
            plt.plot(step, cum_reg.mean(axis=1), linestyle=style, **line_kwargs)

        plt.errorbar(step[sube], cum_reg[sube].mean(axis=1),
                     cum_reg[sube].std(axis=1) / np.sqrt(cum_reg.shape[1]),
                     fmt="none", ecolor=alg[2])

        print(f"{alg[4]} final cumulative regret: {cum_reg.mean(axis=1)[-1]:.2f}")

    plt.title("MNIST Bandit")
    plt.xlabel("Round n")
    plt.ylabel("Cumulative Regret")
    plt.legend(loc="upper left", frameon=False)
    plt.tight_layout()

    plot_path = Path(args.output_dir) / "mnist_bandit_regret.pdf"
    plt.savefig(plot_path)
    plt.close()
    print("Plot saved to", plot_path)

    print("Finished MNIST bandit experiment. Results saved to", args.output_dir)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MNIST linear bandit experiment (10 arms per round, 1 per class) – choose one algorithm with --alg or evaluate all")
    parser.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--n_theta", type=int, default=10000, help="Number of θ samples for training prior (linear)")
    parser.add_argument("--n_mlp", type=int, default=10000, help="Number of MLP parameter samples for neural prior")
    parser.add_argument("--horizon", type=int, default=1000, help="Bandit horizon")
    parser.add_argument("--runs", type=int, default=16, help="Independent runs for evaluation")
    parser.add_argument("--sigma", type=float, default=1.0, help="Reward noise std")
    parser.add_argument("--output_dir", default="Results_mnist", help="Directory to save figures & models")
    parser.add_argument("--dataset_in", type=str, default='./mnist_features_labels.npz', help="Path to pre-extracted MNIST features (.npz)")

    # Diffusion prior hyper-parameters
    parser.add_argument("--T", type=int, default=100)
    parser.add_argument("--alpha", type=float, default=0.99)
    parser.add_argument("--schedule", type=str, choices=["const","cosine"], default="cosine", help="Diffusion noise schedule (cosine recommended)")
    parser.add_argument("--hidden_size", type=int, default=1024)
    parser.add_argument("--epochs", type=int, default=3000)
    parser.add_argument("--batch", type=int, default=256)
    parser.add_argument("--lr", type=float, default=0.0001)

    # SGLD hyper-parameters for inference algorithms
    parser.add_argument("--num_steps_sgld", type=int, default=1)
    parser.add_argument("--step_size_sgld", type=float, default=0.05)
    parser.add_argument("--noise_scale", type=float, default=0.01)
    parser.add_argument("--dps_eta", type=float, default=0.005)
    parser.add_argument("--dmap_eta", type=float, default=0.1)
    parser.add_argument("--dps_acr_eta", type=float, default=0.05)
    parser.add_argument("--h", type=float, default=1.0)
    parser.add_argument("--K_inner", type=int, default=1)

    # seed
    parser.add_argument("--seed", type=int, default=42)

    # Algorithm choice
    parser.add_argument("--alg", type=str, default="all", help="Comma-separated list of algorithms to run. Available: LinTS, LinDiffTS, NeuralTS, NeuralUCB, NeuralDiffDPTS, or use 'all'.")

    args = parser.parse_args()

    main(args)

