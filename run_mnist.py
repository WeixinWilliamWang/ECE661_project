import argparse
import os
from pathlib import Path

# Third-party libraries
import numpy as np
import torch
import matplotlib.pyplot as plt
from typing import Any, Union, Tuple, Optional

import matplotlib as mpl
import umap.umap_ as umap
from sklearn.decomposition import PCA  # Added for PCA visualization
from torch.utils.data import Dataset

from diffusion_TS import DiffusionPriorMLP
from bandit import (
    LinBandit,
    evaluate,
    LinTS,
    LinDiffTS,
    LinDiffDPTS,
    LinDiffDPS,
    LinDiffDMAP,
    NeuralTS,
    NeuralUCB,
    NeuralDiffDPTS,
    NeuralDiffDPS,
    NeuralDiffDMAP
)

import wandb


mpl.style.use("classic")
mpl.rcParams["figure.figsize"] = [6, 4]

mpl.rcParams["axes.linewidth"] = 0.75
mpl.rcParams["errorbar.capsize"] = 3
mpl.rcParams["figure.facecolor"] = "w"
mpl.rcParams["grid.linewidth"] = 0.75
mpl.rcParams["lines.linewidth"] = 0.75
mpl.rcParams["patch.linewidth"] = 0.75
mpl.rcParams["xtick.major.size"] = 3
mpl.rcParams["ytick.major.size"] = 3

mpl.rcParams["pdf.fonttype"] = 42
mpl.rcParams["ps.fonttype"] = 42
mpl.rcParams["font.size"] = 10
mpl.rcParams["axes.titlesize"] = "medium"
mpl.rcParams["legend.fontsize"] = "medium"



mpl.rcParams['savefig.bbox'] = 'tight'
mpl.rcParams['savefig.pad_inches'] = 0



# ---- Imports from training script ----
# Only load_full_mnist is used in this script
from train_MNIST import load_full_mnist

# ---- Utils to load linear / MLP params ----


def flatten_state_dict(sd: dict[str, torch.Tensor]) -> np.ndarray:
    """Flatten state_dict parameters to 1-D numpy array."""
    vec = torch.cat([p.view(-1) for p in sd.values()])
    return vec.cpu().numpy()


def load_mlp_param_matrix(pt_file: Path) -> np.ndarray:
    """Load list of state_dicts and return matrix (N × P)."""
    assert pt_file.exists(), f"MLP params file {pt_file} missing."
    sd_list = torch.load(pt_file, map_location="cpu")
    mat = np.stack([flatten_state_dict(sd) for sd in sd_list]).astype(np.float32)
    return mat






class MNISTNeuralBandit(LinBandit):
    """Bandit where rewards are generated by a binary MLP drawn from neural parameter set.

    For each episode a parameter vector is sampled from `neural_param_matrix` (test split).
    Rewards are sign(MLP(x))."""

    def __init__(self, features: np.ndarray, labels: np.ndarray, sigma: float = 0.0):
        # --- Dataset & dimensions ---
        self.labels = labels.astype(np.int64, copy=False)
        self.all_features = features.astype(np.float32, copy=False)
        self.d = features.shape[1]  # 28*28 = 784
        self.dataset = torch.utils.data.TensorDataset(
            torch.from_numpy(self.all_features),  # (N, 784) float32
            torch.from_numpy(self.labels),        # (N,)     int64
        )

        # --- Bandit arms configuration ---
        self.K = 100  # number of arms per round

        # --- Positive class (chosen once per environment) ---
        self.pos_class = int(np.random.randint(0, 10))

        self.sigma = sigma

        # Now that we know K and d, call parent constructor with placeholder X, theta
        super().__init__(np.zeros((self.K, self.d)), np.zeros(self.d), sigma=sigma, labels=self.labels)


    def randomize(self):
        # ----- Choose 10 images per class (0-9) so that each class appears equally among the K=100 arms -----
        per_class = self.K // 10  # =10
        arms_idx_list = []
        for cls in range(10):
            cls_indices = np.where(self.labels == cls)[0]
            # Randomly pick `per_class` indices for this class (no replacement)
            chosen = np.random.choice(cls_indices, size=per_class, replace=False)
            arms_idx_list.extend(chosen)

        # Shuffle the aggregated indices to mix class ordering
        rng = np.random.default_rng()
        arms_idx = rng.permutation(np.array(arms_idx_list))

        # Gather the selected samples into numpy arrays expected downstream
        X_list = []
        labels_list= []
        for idx in arms_idx:
            feat, lbl = self.dataset[int(idx)]  # tensors
            X_list.append(feat.numpy())
            labels_list.append(int(lbl))

        self.X = np.stack(X_list, axis=0)  # shape (K, d)
        arm_labels = np.array(labels_list, dtype=np.int64)

        # Reward structure: +1 for positive class, -1 otherwise
        self.mu = np.where(arm_labels == self.pos_class, 1.0, -1.0)

        # Optionally add Gaussian noise
        if self.sigma > 0:
            self.rt = self.mu + self.sigma * np.random.randn(self.K)
        else:
            self.rt = self.mu.copy()

        # Identify best arm (any with +1, choose first)
        pos_indices = np.where(arm_labels == self.pos_class)[0]
        self.best_arm = int(pos_indices[0]) if pos_indices.size > 0 else 0


def prepare_mnist_pixels():
    """Load raw MNIST pixel vectors (flattened 784)."""
    images, labels = load_full_mnist(flatten=True)
    return images.numpy().astype(np.float32), labels.numpy().astype(np.int64)


# -----------------------------------------------------------------------------
# Visualization helper
# -----------------------------------------------------------------------------


def visualize_prior_vs_true(true_mat: np.ndarray, prior_model, out_path: Path, n_vis: int = 1000):
    """UMAP scatter plot comparing true parameter samples vs diffusion prior."""
    umap_skip = False
    if out_path.exists():
        print(f"[Skip] {out_path.name} already exists, skipping UMAP plot.")
        umap_skip = True

    n_vis = min(n_vis, true_mat.shape[0])
    true_subset = true_mat[:n_vis]
    prior_subset = prior_model.sample(n_vis)[0]

    reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.1, random_state=0)
    true_2d = reducer.fit_transform(true_subset)
    prior_2d = reducer.transform(prior_subset)

    if not umap_skip:
        plt.figure(figsize=(6, 6))
        plt.scatter(true_2d[:, 0], true_2d[:, 1], s=6, marker=".", color="b", label="True Prior")
        plt.scatter(prior_2d[:, 0], prior_2d[:, 1], s=6, marker=".", color="grey", label="Diffusion Prior")
        plt.tight_layout()
        plt.legend()
        plt.savefig(out_path)
        plt.close()

    # ------------------ PCA visualization (new) ------------------
    # Equivalent scatter plot using first two principal components
    pca_out_path = out_path.parent / f"{out_path.stem}_pca{out_path.suffix}"

    if pca_out_path.exists():
        print(f"[Skip] {pca_out_path.name} already exists, skipping PCA plot.")
        return

    from sklearn.decomposition import PCA  # imported above too but local import for safety

    pca = PCA(n_components=2, random_state=0)
    true_pca = pca.fit_transform(true_subset)
    prior_pca = pca.transform(prior_subset)

    plt.figure(figsize=(6, 6))
    plt.scatter(true_pca[:, 0], true_pca[:, 1], s=6, marker=".", color="b", label="True Prior")
    plt.scatter(prior_pca[:, 0], prior_pca[:, 1], s=6, marker=".", color="grey", label="Diffusion Prior")
    plt.tight_layout()
    plt.legend()
    plt.savefig(pca_out_path)
    plt.close()


# -----------------------------------------------------------------------------
# Utility: ensure we have a cached NPY archive of MNIST pixel features (no mmap).
# Returns (features, labels) as regular numpy arrays; regenerates cache if missing
# or incomplete.
# -----------------------------------------------------------------------------


def _get_mnist_feature_cache(ds_path: str):
    """Return (features, labels) arrays from ds_path, regenerating if needed."""
    if os.path.exists(ds_path):
        with np.load(ds_path) as npz:
            if {"features", "labels"}.issubset(npz.files):
                print(f"[Cache] Loaded MNIST features from {ds_path}")
                return npz["features"], npz["labels"]
        print(f"[Warning] {ds_path} missing keys. Regenerating...")
        Path(ds_path).unlink(missing_ok=True)

    # (Re)generate dataset
    images_px, lbls_px = load_full_mnist(flatten=True)
    features = images_px.numpy().astype(np.float32)
    labels = lbls_px.numpy().astype(np.int64)
    np.savez(ds_path, features=features, labels=labels)
    print(f"Saved MNIST features to {ds_path}")
    return features, labels


# ----------------- Plot helpers -----------------
def linestyle2dashes(style: str) -> Union[str, Tuple[int, ...]]:
    """Map matplotlib linestyle to dash pattern or return the style itself.

    Returns a tuple for custom dash sequences that can be passed to
    Line2D.set_dashes(). For solid lines returns "-" etc.
    """
    mapping = {
        "-": "-",           # solid
        "--": (6, 2),        # dashed
        "-.": (4, 2, 1, 2),  # dash-dot
        ":": (1, 2),         # dotted
    }
    return mapping.get(style, style)

def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main(args):
    set_seed(args.seed)
    # set paths (using Path objects for convenient methods such as .exists())
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    lin_theta_path = output_dir / "mnist_linear_thetas.npy"
    mlp_param_path = output_dir / "mnist_mlp_state_dicts.pt"

    results_dir = output_dir / "eval_results"
    results_dir.mkdir(parents=True, exist_ok=True)

    # ---- linear θ matrix ----
    if lin_theta_path.exists():
        print(f"Loading linear θ samples from {lin_theta_path}")
        theta_samples = np.load(lin_theta_path)
        d_linear = theta_samples.shape[1]
        d_feat = d_linear  # feature dimension now equals θ dimension (using raw pixels)

        # Always load raw pixel data to form bandit arms (no mmap)
        ds_path = os.path.join(args.output_dir, "mnist_features_labels.npz")
        features_all, labels_all = _get_mnist_feature_cache(ds_path)
        # Try to load positive‐class labels if available (for visualization)
        pos_path = Path(args.output_dir) / "pos_labels.npy"
        pos_labels = np.load(pos_path) if pos_path.exists() else None
    else:
        raise FileNotFoundError("Please run train_MNIST.py to generate linear parameter file.")

    # ---------------- train / test split (configurable fraction) ----------------
    perm = np.random.permutation(theta_samples.shape[0])
    n_train_lin = int(theta_samples.shape[0] * args.train_frac)
    theta_train = theta_samples[perm[:n_train_lin]]
    theta_test = theta_samples[perm[n_train_lin:]]


    # ----- Linear prior -----
    lin_prior_path = output_dir / "diffusion_prior_linear_sd.pt"
    device = args.device
    # Initialize linear diffusion prior (use simple MLP eps-net; Transformer here
    # would be dimensionally incompatible with the 1-layer linear θ representation)
    lin_prior = DiffusionPriorMLP(
        d=d_feat,
        T=args.T,
        alpha=args.alpha,
        hidden=args.hidden_size,
        device=device,
        schedule=args.schedule,
        arch="mlp",
    )

    if lin_prior_path.exists():
        print(f"[Cache] Loading linear DiffusionPrior state_dict from {lin_prior_path}")
        state_dict = torch.load(lin_prior_path, map_location=device)
        lin_prior.load_state_dict(state_dict)
    else:
        lin_prior.train(
            theta_train,
            epochs=args.epochs,
            batch=args.batch,
            lr=args.lr,
            log_prefix="linear_prior",
        )
        torch.save(lin_prior.state_dict(), lin_prior_path)


    if d_feat >= 2:
        visualize_prior_vs_true(
            theta_test,
            lin_prior,
            Path(args.output_dir) / "mnist_diffusion_prior.pdf",
        )

    # ----- Neural (MLP) prior -----
    neural_mat = load_mlp_param_matrix(mlp_param_path)
    P = neural_mat.shape[1]

    # ---- train / test split first for neural parameters ----
    perm2 = np.random.permutation(neural_mat.shape[0])
    n_train_neural = int(neural_mat.shape[0] * args.train_frac)
    neural_train = neural_mat[perm2[:n_train_neural]]
    neural_test  = neural_mat[perm2[n_train_neural:]]


    neural_prior_path = Path(args.output_dir) / "diffusion_prior_mlp_sd.pt"

    # Common initializer
    neural_prior = DiffusionPriorMLP(
        d=P,
        T=args.T,
        alpha=args.alpha,
        hidden=args.hidden_size,
        device=args.device,
        schedule=args.schedule,
        arch="mlp",
    )

    if neural_prior_path.exists():
        print(f"[Cache] Loading neural DiffusionPrior state_dict from {neural_prior_path}")
        state_dict = torch.load(neural_prior_path, map_location=args.device)
        neural_prior.load_state_dict(state_dict)
    else:
        # --------- Visualize untrained neural prior vs true parameters ---------
        visualize_prior_vs_true(
            neural_test,
            neural_prior,
            Path(args.output_dir) / "mnist_mlp_diffusion_prior_before_train.pdf",
        )

        neural_prior.train(
            neural_train,
            epochs=args.epochs,
            batch=args.batch,
            lr=args.lr,
            log_prefix="mlp_prior",
        )
        torch.save(neural_prior.state_dict(), neural_prior_path)

    ######################## 2.5) Visualize prior vs true θ ###################
    if args.visualize:
        # ----------- Visualize Linear θ parameter space -----------
        visualize_prior_vs_true(
            theta_test,
            lin_prior,
            Path(args.output_dir) / "mnist_diffusion_prior.pdf",
        )

        # ----------- Visualize MLP parameter space -----------
        visualize_prior_vs_true(
            neural_test,
            neural_prior,
            Path(args.output_dir) / "mnist_mlp_diffusion_prior.pdf",
        )

    ######################## 3) Bandit evaluation #############################
    envs = [MNISTNeuralBandit(features_all, labels_all, sigma=args.sigma) for _ in range(args.runs)]

    # Define all algorithms & their params

    neural_common = {
        "hidden": args.hidden_neural,
        "nu": args.nu,
        "batch_size": args.batch_size,
        "reduce": None,
        "reg": args.reg,
        "device": args.device,
    }

    neural_ts_params  = neural_common.copy()
    neural_ucb_params = neural_common.copy()

    full_alg_list = {
        "LinTS": ("LinTS", {"sigma": args.sigma}, "tab:blue", "--", "TS"),
        "LinDiffTS": (
            "LinDiffTS",
            {
                "prior": lin_prior,
                "sigma": args.sigma,
                "theta0": np.zeros(d_feat),
                "Sigma0": 1e6 * np.eye(d_feat),
            },
            "tab:orange",
            "-",
            "DiffTS",
        ),
        "NeuralTS": ("NeuralTS", neural_ts_params, "tab:green", "--", "NeuralTS"),
        "NeuralUCB": ("NeuralUCB", neural_ucb_params, "tab:red", "-.", "NeuralUCB"),
        "NeuralDiffDPTS": (
            "NeuralDiffDPTS",
            {
                "prior": neural_prior,
                "sigma": args.sigma,
                "num_steps_sgld": args.num_steps_sgld,
                "step_size_sgld": args.step_size_sgld,
                "noise_scale": args.noise_scale,
            },
            "tab:purple",
            "-",
            "DLTS",
        ),
        "NeuralDiffDMAP": (
            "NeuralDiffDMAP",
            {
                "prior": neural_prior,
                "sigma": args.sigma,
                "K_inner": args.K_inner,
                "eta": args.dmap_eta,
            },
            "tab:olive",
            "--",
            "DMAP",
        ),

    }

    available_algs = list(full_alg_list.keys())
    alg_arg = args.alg.strip().lower()
    if alg_arg == "all":
        selected_alg_keys = available_algs
    else:
        requested = [item.strip() for item in args.alg.split(",") if item.strip()]
        invalid = sorted(set(requested) - set(available_algs))
        if invalid:
            raise ValueError(
                f"Unknown algorithms requested: {invalid}. Available options: {available_algs}"
            )
        selected_alg_keys = requested

    algs = [full_alg_list[name] for name in selected_alg_keys]

    step = np.arange(1, args.horizon + 1)
    # subset indices where we show error bars
    sube = np.linspace(0, args.horizon - 1, num=10, dtype=int)

    for alg in algs:
        alg_class = globals()[alg[0]]
        # Log which algorithm starts to help locate memory bottlenecks
        print(f"[RUN] {alg[0]} ({alg[4]})", flush=True)
        # Run evaluation sequentially
        regret, _ = evaluate(alg_class, alg[1], envs, args.horizon, printout=True)
        print(f"[DONE] {alg[0]} ({alg[4]})", flush=True)

        cum_reg = regret.cumsum(axis=0)

        # Persist evaluation results for later analysis/plotting
        alg_name = alg[0]
        # Build filename suffix from tunable (primitive) parameters for the current algorithm
        simple_params = {k: v for k, v in alg[1].items() if isinstance(v, (int, float, str, bool))}
        param_suffix = "_".join([f"{k}{simple_params[k]}" for k in sorted(simple_params)])
        result_path = (
            results_dir / f"{alg_name}_{param_suffix}_results.npz" if param_suffix else results_dir / f"{alg_name}_results.npz"
        )
        np.savez_compressed(
            result_path,
            regret=regret,
            cumulative_regret=cum_reg,
            step=step,
            algorithm=np.array(alg_name),
            display_label=np.array(alg[4]),
            runs=np.array(args.runs, dtype=np.int32),
            horizon=np.array(args.horizon, dtype=np.int32),
            sigma=np.array(args.sigma, dtype=np.float32),
        )

        style = linestyle2dashes(alg[3])

        line_kwargs = dict(color=alg[2], label=alg[4], linewidth=2)
        if isinstance(style, tuple):
            line, = plt.plot(step, cum_reg.mean(axis=1), **line_kwargs)
            line.set_dashes(style)
        else:
            plt.plot(step, cum_reg.mean(axis=1), linestyle=style, **line_kwargs)

        plt.errorbar(step[sube], cum_reg[sube].mean(axis=1),
                     cum_reg[sube].std(axis=1) / np.sqrt(cum_reg.shape[1]),
                     fmt="none", ecolor=alg[2])

        # ----- Print final cumulative regret statistics -----
        final_mean = cum_reg.mean(axis=1)[-1]
        final_var = cum_reg.var(axis=1)[-1]
        print(f"{alg[4]} final cumulative regret: {final_mean:.2f} (variance={final_var:.2f})")

    plt.title("MNIST Bandit")
    plt.xlabel("Round n")
    plt.ylabel("Cumulative Regret")
    plt.legend(loc="upper left", frameon=False)
    plt.tight_layout()

    plot_path = os.path.join(args.output_dir, "mnist_bandit_regret.pdf")
    plt.savefig(plot_path)
    plt.close()
    print("Plot saved to", plot_path)

    print("Finished MNIST bandit experiment. Results saved to", args.output_dir)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="MNIST linear bandit experiment (10 arms per round, 1 per class) – choose one algorithm with --alg or evaluate all")
    parser.add_argument("--device", default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--n_theta", type=int, default=10000, help="Number of θ samples for training prior (linear)")
    parser.add_argument("--n_mlp", type=int, default=10000, help="Number of MLP parameter samples for neural prior")
    parser.add_argument("--horizon", type=int, default=1000, help="Bandit horizon")
    parser.add_argument("--runs", type=int, default=5, help="Independent runs for evaluation")
    parser.add_argument("--sigma", type=float, default=0, help="Reward noise std")
    parser.add_argument("--output_dir", default="Results_mnist", help="Directory to save figures & models")
    parser.add_argument("--dataset_in", type=str, default='./mnist_features_labels.npz', help="Path to pre-extracted MNIST features (.npz)")

    # Diffusion prior hyper-parameters
    parser.add_argument("--T", type=int, default=100)
    parser.add_argument("--alpha", type=float, default=0.99)
    parser.add_argument("--schedule", type=str, choices=["const","cosine","linear"], default="linear", help="Diffusion noise schedule (cosine recommended)")
    parser.add_argument("--hidden_size", type=int, default=1024)
    parser.add_argument("--epochs", type=int, default=50000)
    parser.add_argument("--batch", type=int, default=256)
    # train/test split fraction
    parser.add_argument("--train_frac", type=float, default=0.8, help="Fraction of samples used for training priors (remainder for test)")
    parser.add_argument("--lr", type=float, default=0.001)

    # Neural algorithm hyper-parameters
    parser.add_argument("--hidden_neural", type=int, default=128)
    parser.add_argument("--nu", type=float, default=0.1)
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--reg", type=float, default=0.1)

    # SGLD hyper-parameters for inference algorithms
    parser.add_argument("--num_steps_sgld", type=int, default=1)
    parser.add_argument("--step_size_sgld", type=float, default=0.05)
    parser.add_argument("--noise_scale", type=float, default=0.01)
    parser.add_argument("--dps_eta", type=float, default=0.005)
    parser.add_argument("--dmap_eta", type=float, default=0.1)
    parser.add_argument("--dps_acr_eta", type=float, default=0.05)
    parser.add_argument("--h", type=float, default=1.0)
    parser.add_argument("--K_inner", type=int, default=1)

    # seed
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--visualize", action="store_true", default=False, help="Visualize prior vs true θ")
    # Algorithm choice
    parser.add_argument("--alg", type=str, default="all", help="Comma-separated list of algorithms to run. Available: LinTS, LinDiffTS, NeuralTS, NeuralUCB, NeuralDiffDPTS, or use 'all'.")

    args = parser.parse_args()

    wandb.init(project="mnist-bandit", name=args.alg, config=args)

    main(args)
    # Ensure all pending wandb logs are flushed before the script exits
    wandb.finish()

